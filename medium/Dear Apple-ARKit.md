# DEAR APPLE, ARKIT IS LOOKING IN THE WRONG DIRECTION

https://medium.com/@magesteve/dear-apple-arkit-is-looking-in-the-wrong-direction-e71172340414

## Dear Apple,
## Please give us an ARKit that looks outward, so we can place augmented graphics on locations, not one that allows us to put things on a table.
### Thank you, A Concerned Developer.

Please do not misunderstand. Apple ARKit is the perfect example of an Apple-provided API service. The Apple SDK manages the camera, the inertial/magnetic positioning calculations and the flat surface detection required for this type of augmented reality. Otherwise, every AR app would have a lot of nearly identical code that programmers would have to write themselves. Now developers will only have to concern themselves with their app-specific content.
Apple’s ARKit is a fantastic starting point for writing great apps!
It is also facing the wrong direction.
For those unfamiliar with the term Augmented Reality (and the related term Mixed Reality), AR involves projecting computer generated content (usually graphics) onto the real world. Unlike Virtual Reality, where the user immerses himself into an entirely computer-generated environment, AR places a small amount of graphics on top of a view of the real world. Some hardware does this with expensive AR glasses, while Apple’s solution has people looking at the screen of an iOS device which shows what the video camera is pointed at.
The placement of the graphics is vital for Augmented Reality. If you just put the computer art anywhere, there is no feeling that the graphics are enhancing the user’s reality. The art is only on top of the screen. ARKit gives the impression that some 3D object has been set on some flat surface. Once the flat surface (usually a table or the floor) is found, and a virtual 3D object is placed, when the user walks around the object, the image of the object rotates, just as if the object was really there. Backing up makes it grow smaller, moving closer expands it. If you look completely away from that surface, the object usually disappears, until you place it somewhere else. This new reality is done with a bit of coding magic, using the inertial hardware of the device, and the camera’s angle of view on the flat surface.
As mentioned, this is great for putting an object, like a chess board, on a surface like a table. The most useful ARKit I have seen was a demo to place furniture in an empty room, so you could move around the room, and see how it would look with the new pieces. The best example I have seen of this type of AR outside of Apple can be found at Lego stores. Placing some of the boxes of the Lego pieces in front of the camera/screen system, projects a view of the completed Lego piece on top of the box. You can rotate the box to see all angles of the assembled animated Lego piece on the screen.
Now that you understand ARKit, take a look at some films or TV shows that show other examples of Augmented Reality. Watch Minority Report, or Iron Man, or Terminator, or Mission Impossible (the list of Sci-Fi shows with VR/AR is rather long). ARKit is not the type of Augmented Reality that is often shown. The vast majority of the time, a different type of AR (sometimes called HUD AR or Heads Up Display AR) can be found. When looking at an object, or a location, additional information is overlaid with the view of reality. The Terminator looks at a vehicle, and gets the description of how to blow it up. Iron Man looks at a building, and views information about the content of the structure.
This outward-looking AR is not concerned about placing objects in the view. Instead, it deals with adding information (often called Tags) on top of the image of the world the user sees. If a user turns in any direction, his orientation and his location determine what additional computer enhancements he sees.
This is not Science Fiction. It is possible now! An iOS device would not have the power or software to do object recognition, but it knows its GPS location as well as the direction and tilt of the Camera. Imagine an app that shows you the path, and distance, to the nearest Starbucks, based on which direction you are facing. Or one that could tell you the name of the mountain you are viewing.
This type of augmented reality app already exists in the App store. Most people have seen the astronomy app that will tell you the name of the constellation or star you are viewing when you tilt the screen toward the sky.
Wouldn’t it be nice if app developers did not have to write the code that managed the camera, GPS calculations, and inertial/magnetic APIs? Nor write the code to calculate where the Tags have to appear on the device’s screen? Then the developer would only have to plug its database of essential locations into the SDK, as well as describe how the Tags should appear, and the Apple manager would handle the rest.
There is a parallel in the past describing these two different views of reality. Apple’s original QuickTime VR (released in 1995) provided two types of views: a Panorama View that showed a 360 degree animated movie and the Object View where the user was looking at an object that could be viewed from every angle. Usually, for Quicktime VR, the views were not computer generated, but real photos taken in all directions (either outward for Panoramas, or inward for Objects).
It is worth noting that by far the more common use of QTVR was displaying Panoramas. Object Views were rarely used, while the QuickTime VR Panoramas View is the direct ancestor of the 360 VR film standard that has become so popular. People want to look out at the world, not in towards a specific object.
So please Apple, give me an ARKit that looks in the right direction.
A Concerned Developer
Ps. WWDC 2018 would be a perfect opportunity to announce this!
Steve Sheets, June 3, 2018
Email: magesteve@mac.com
